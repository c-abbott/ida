---
title: |
  <center> Incomplete Data Analysis </center>
  <center> Assignment 1 </center>
author: "Callum Abbott"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
load("databp.Rdata")
```
# Code
The Rmd file used to create this pdf can be found in the following repo https://github.com/c-abbott/ida
under the sub-directory assignment-1/ida-assignment-1.Rmd

# Question 1
Let us suppose that on a (hypothetical) survey there is a question about alcohol consumption
and that the variable ALQ records the respondent’s answer to the following specific question:
“In the past year, have you had at least 12 drinks of any type of alcoholic beverage?”. The
possible answers are ‘Yes’ or ‘No’. Not all participants respond to this question, that is,
the ALQ variable has some missing values. Further, and again hypothetically, suppose that
we only have additional data on the gender of the participants in the survey (which is fully
observed). For each of the following situations, choose, justifying, the correct answer.

## Question (1a)
* **Description**: Suppose that ALQ is MCAR. The probability of ALQ being missing 
for those with ALQ=Yes is 0.3. What is the probability of ALQ being missing for
those with ALQ=No?

* **Answer**: In our hypothetical survey about alcohol consumption two variables 
are recorded: ALQ and gender. We note that the ALQ variable contains some missing
values whilst the gender variable is fully observed. We now suppose that ALQ is MCAR, 
indicating that the missingness of the ALQ variable is independent of any data. 
This implies that the probability of the ALQ being missing for those with ALQ=No
must **also be 0.3**. If it were any other value, the probability of the data being
missing would then have an implied dependence on the ALQ variable rendering the data
to **not** be MCAR.

## Question (1b)
* **Description**: ALQ being MAR given gender means:

* **Answer**: (ii) The probability of ALQ being missing is independent of the Yes/No
value of ALQ after adjusting for gender.

## Question (1c)
* **Description**: Suppose again that ALQ is MAR given gender, and that the probability
of ALQ being missing for men is 0.1. What is the probability of ALQ being missing 
for women?

* **Answer**: (iii) It is impossible to conclude from the information given. 
I believe this because answer (i) of 0.1 would imply that the data would *actually* 
be MCAR. Meanwhile, answer (ii) seems to indicate that some conservation of probability 
must be present which is incorrect to assume. This leaves answer (iii). 

# Question 2
* **Description**: Suppose that a dataset consists of 100 subjects and 10 variables.
Each variable contains 10% of missing values. What is the largest possible subsample 
under a complete case analysis? What is the smallest? Justify.

* **Answer - Largest CCA Subset:** Let us begin with first defining how our data 
is presented and stored. We can view the data in a table format with 100 rows and 
10 columns. The question does not state the total number of missing values in our 
dataset of $100 \times 10 = 1000$ values, and so let us first assume a scenario where 
we have 10 missing values (out of 1000) in our dataset. In order to satisfy the
criterion that "*each variable (column) contains 10% of missing values*", then we 
must have 1 missing value in each of our 10 variables. We know that in a complete 
case analysis that if any one of a subject's 10 variables contains a missing value,
then that subject will not be included in the analysis. Therefore, if all our 10 
missing values align along a single row in our dataset, then we would only have to
discard a single subject from our complete case analysis leaving a total of 
**99 subjects**.

* **Answer - Smallest CCA Subset:** We now imagine the converse situation where we
have a much greater number of missing values in our dataset. For argument's sake, 
let us propose that this number is now 100. This time, in order to satisfy the criterion
that "*each variable (column) contains 10% of missing values*", we must have 10 
missing values in each of our 10 variables. Now suppose that rows 1-10 were missing
for variable 1, rows 11-20 were missing for variable 2, rows 21-30 were missing 
for variable 3 and so on up to variable 10. This kind of structure in our dataset 
would result in a missing value being present for every subject in our dataset 
leaving a total of **0 subjects** under a complete case analysis.

# Question 3
* **Description**: Consider a two variable $(Y_1, Y_2)$ problem, with each variable 
defined as follows:
\begin{equation*}
Y_1 = 1+ Z_1, \;
Y_2 = 5 + 2 \times Z_1 + Z_2
\end{equation*}
where $Y_1$ is fully observed but $Y_2$ is subject to missingness. Further consider
that $Y2$ is missing if $a \times (Y_1 - 1) + b \times (Y_2 - 5) + Z_3 < 0$, where
$Z_1$, $Z_2$, and $Z_3$ follow independent standard normal (that is, mean 0 and
variance 1) distributions.

## Question (3a)
* **Description**: Start by simulating a (complete) dataset of size 500 on $(Y_1, Y_2)$.
Then, and considering $a = 2$ and $b = 0$, simulate the corresponding observed dataset
(by imposing missingness on $Y_2$ as instructed above). Is this mechanism MCAR,
MAR, or MNAR? Display the marginal distribution of $Y_2$ for the complete (as 
originally simulated) and observed (after imposing missingness) data. Comment.

* **Answer**: Given that we know $Y_1$ is completely observed and $Y_2$ is only 
partially observed, we see that our missingness indicator, $r$, for the parameters 
$a=2$ and $b=0$, is only dependent on $Y_1$. This, by definition, tells us that 
the missing data for $Y_2$ will be MAR. Our beliefs are then confirmed after 
observing that the distributions (density and box) of the complete, missing and
observed data of $Y2$ are **not congruent** (see below).

However, one cannot rule out the missing data mechanism being MNAR since $Y_1 = 1 + Z_1$ and
$Y_2 = 5 + 2 \times Z_1 + Z_2$. This means that the missingness in $Y_2$ **could**
also result in missingness in $Z_1$ and hence $Y_1$ too. This depends on how the 
missing information is distributed across the $Z_1$ and $Z_2$ variables which we have no 
knowledge of. In summary, if the cause of the missingness in $Y_2$ is confined to the $Z_2$
variable then the missing data is MAR. If the cause the of the missignness in
$Y_2$ is spread across $Z_2$ and $Z_1$, the missing data is MNAR. We should bear
this in mind as we continue our analysis.

## Helper Functions
```{r helper_funcs}
plot_densities = function(data1, data2, ylim = NULL) {
  # Calculate y limit of both densities
  ylim = if (is.null(ylim)) range(d1$d$y, d2$d$y) else ylim
  
  # Plotting
  plot(data1$d, lwd = 2, col = data1$col, xlab = expression(Y[2]), ylim = ylim, main = NA)
  lines(data2$d, lwd = 2, col = data2$col)
  legend('topleft', 
    legend = c(data1$name, data2$name),
    col = c(data1$col, data2$col),
    lty = c(1, 1), lwd = c(2, 2), bty = 'n'
  )
}

# Helper function for calculating the mean and standard error on a dataset
get_statistics =  function(data) {
  # Create an empty list to store the statistics
  results = NULL
  # Computing CCA statistics
  results$mean = mean(data, na.rm = TRUE)
  results$se = sd(data, na.rm = TRUE) / sqrt(sum(Rfunc(a=2, b=0)))
  results$sd = sd(data, na.rm = TRUE)
  return(results)
}
```

## $Y_2$ Marginal Densities
```{r 3a}
# Simulating Gaussian data
nsim = 500
Z1 = rnorm(500, 0, 1)
Z2 = rnorm(500, 0, 1)
Z3 = rnorm(500, 0, 1)

# Define Y1 and Y2 from Z samples
Y1 = 1 + Z1
Y2 = 5 + 2 * Z1 + Z2
Y2_miss = 5 + 2 * Z1 + Z2

# Create missingness indicator for Y2
Rfunc = function(a, b) { a * (Y1 - 1) + b * (Y2 - 5) + Z3 >= 0 }
R = Rfunc(a = 2, b = 0)

# Impose missingness
Y2_miss[!R] = NA
Y2_obs = Y2[R]

# Plot densities for the complete and observed data (after imposing missingness) for Y2
plot_densities(
  list(d = density(Y2), name = 'Complete data', col = 'blue'), 
  list(d = density(Y2_obs), name = 'Observed data', col = 'red'),
  ylim = c(0, 0.30)
)
```

## Question (3b)
* **Description**: For the observed dataset simulated in (a), impute the missing
values using stochastic regression imputation. Display the marginal distribution
of Y2 for the complete (as originally simulated) and completed (after imputation) 
data. Comment. 

* **Answer**: We see that both the complete data density plot, and the density 
plot after imposing missingness and then imputing the missing values using stochastic 
regression imputation, are very similar. The only distinguishing difference between 
the two is that the "Data w/ SRI" density plot is more noisy and less smooth than 
the complete data density plot as one would expect. I have quantified the similarity
between these distributions by computing the mean of both distributions for which 
I observe their means to both be $\approx 5$. I have also 
computed the standard deviations for both distributions and observe the standard
deviation of `Y2_sri` to be slightly larger (2.31) than the complete `Y2` (2.24)
distribution which is reflective of the stochastic noise we added to `Y2_sri`. 
Overall, I believe stochastic regression imputation (SRI) has performed well at imputing the missing data values 
due to the relatively unbiased estimates of the mean and standard deviation. This therefore confirms that the missing data mechanism we are working with is actually MAR due to the SRI's good performance.

```{r 3b}
# Defining variables needed for SRI
Y1_miss = Y1[!R]
n_miss = sum(!R)
Y2_sri = Y2_miss

# Conducting SRI
fits  = lm(Y2_miss ~ Y1)
# Noise for stochastic regression imputation
noise = rnorm(n_miss, mean = 0, sd = sigma(fits))
Y2_sri[!R] = predict(fits, data.frame(Y1 = Y1_miss)) + noise

# Plot densities for complete and SRI imputed data
plot_densities(
  list(d = density(Y2), name = 'Complete data', col = 'blue'), 
  list(d = density(Y2_sri), name = 'SRI Imputed data', col = 'red'),
  ylim = c(0, 0.27)
)

# Get statistics
Y2_sri_stats = get_statistics(Y2_sri)

# Print statistics for comparison
cat(sprintf("Complete Mean: %f\nComplete Standard Deviation %f\n", mean(Y2), sd(Y2)))

cat(sprintf("SRI Mean: %f\nSRI Standard Error: %f\nSRI Standard Deviation: %f\n", 
            Y2_sri_stats$mean, Y2_sri_stats$se, Y2_sri_stats$sd))
```

## Linear Model Check
We also check that our linear model fit on Y2 with Y1 is valid before moving on.
The final regression formula is observed to be $Y_2 = 2.87014 + 2.00000 * Y_1$ 
where both coefficients are significant to a level of 0.01 indicating a valid
linear model.
Furthermore, we also observe no correlation between the residuals and the 
fitted values, in addition to reasonable congruence in the q-q plot all of which
futher enforces the validity of our linear model.

```{r, include = TRUE, message = FALSE}
# Summary of model
summary(fits)
# Regression plot
plot(fits$fitted.values, residuals(fits), xlab = "Fitted values", ylab = "Residuals")
# q-q plot
qqnorm(rstandard(fits), xlim = c(-3, 3), ylim = c(-3, 3))
qqline(rstandard(fits), col = 2)
```

## Question (3c)
* **Description**: Using the complete dataset simulated in (a), now impose 
missingness on Y2 by considering $a = 0$ and $b = 2$. Is this mechanism MCAR, 
MAR, or MNAR? Display the marginal distribution of Y2 for the complete (as 
originally simulated) and observed (after imposing missingness) data. Comment.

* **Answer**: Given that we know $Y_1$ is completely observed and $Y_2$ is only 
partially observed, we see that our missingness indicator, $r$, for the parameters
$a=0$ and $b=2$, is only dependent on $Y_2$. This, by definition, tells us that
the missing data for $Y_2$ will be MNAR since the missingess depends on the missing
variable itself. Our beliefs are then confirmed after observing that the distributions
(density and box) of the complete, missing and observed data of $Y2$ are **even
less congruent** than the plots seen in 3(a) (see below).

The plots show that the density of $Y_2 < 5$ decreased significantly and was redistributed over higher values of $Y_2$ when missingness was imposed. We can explain this behavior by recalling the missingness indicator formula: $r = a \times (Y_1 - 1) + b \times (Y_2 - 5) + Z_3 < 0$.  We see that if $Y_2 < 5$, then the value of $r$ is reduced which in turn increases the probability of those values being missing.

```{r 3c}
# Define Y1 and Y2 from Z samples
Y1 = 1 + Z1
Y2 = 5 + 2 * Z1 + Z2
Y2_miss = 5 + 2 * Z1 + Z2

# Create missingness indicator for Y2
R = Rfunc(a = 0, b = 2)

# Impose missingness
Y2_miss[!R] = NA
Y2_obs = Y2[R]

# Plot densities for the complete and observed data (after imposing missingness) for Y2
plot_densities(
  list(d = density(Y2), name = 'Complete data', col = 'blue'), 
  list(d = density(Y2_obs), name = 'Observed data', col = 'red'),
  ylim = c(0, 0.30)
)
```

## Question (3d) 
* **Description:** The same as in (b) but for the observed data generated in (c). 

* **Answer**: Now that the missingness of the variable $Y_2$ is MNAR, 
we observe significant deviations of the SRI imputed dataset (mean = 5.72638, 
sd = 1.814302) from the complete dataset (mean = 4.999348, sd = 2.244424). This
indicates that SRI produces **biased estimates** of missing values for MNAR data
and so we should look towards methods such as MICE in order to reduce these biases.
```{r 3d}
# Defining variables needed for SRI
Y1_miss = Y1[!R]
n_miss = sum(!R)

# Noise for stochastic regression imputation
noise = rnorm(n_miss, mean = 0, sd = sigma(fits))

# Conducting SRI
fits  = lm(Y2_miss ~ Y1)
Y2_sri = Y2_miss
Y2_sri[!R] = predict(fits, data.frame(Y1 = Y1_miss)) + noise

# Plot densities for complete and SRI imputed data
plot_densities(
  list(d = density(Y2), name = 'Complete data', col = 'blue'), 
  list(d = density(Y2_sri), name = 'SRI Imputed data', col = 'red'),
  ylim = c(0, 0.27)
)
Y2_sri_stats = get_statistics(Y2_sri)

# Print statistics for comparison
cat(sprintf("Complete Mean: %f\nComplete Standard Deviation %f\n", mean(Y2), sd(Y2)))

cat(sprintf("SRI Mean: %f\nSRI Standard Error: %f\nSRI Standard Deviation: %f\n", 
            Y2_sri_stats$mean, Y2_sri_stats$se, Y2_sri_stats$sd))
```

# Question 4
* **Description:** It is sometimes necessary to lower a patient’s blood pressure 
during surgery, using a hypotensive drug. Such drugs are administrated continuously 
during the relevant phase of the operation; because the duration of this phase 
varies, so does the total amount of drug administered. Patients also vary in the 
extent to which the drugs succeed in lowering blood pressure. The sooner the blood 
pressure rises again to normal after the drug is discontinued, 2
the better. The dataset databp.Rdata available on Learn, a partial missing value
version of the data presented by Robertson and Armitage (1959), relate to a particular
hypotensive drug and give the time in minutes before the patient’s systolic blood
pressure returned to 1000mm of mercury (the recovery time), the logarithm (base 10) 
of the dose of drug in milligrams (you can use this variable as is, no need to 
transform it to the original scale), and the average systolic blood pressure 
achieved while the drug was being administered.

## Question (4a)
* **Description:** Carry out a complete case analysis to find the mean value of 
the recovery time (and associated standard error) and to find also the (Pearson) 
correlations between the recovery time and the dose and between the recovery time 
and blood pressure.

* **Answer:** 
```{r helper_funcs_4}
# Missingness indicator for the recovery time variable
R_func = function(data) {!is.na(data$recovtime)}
R = R_func(databp)

get_statistics = function(data, na.rm=TRUE, method) {
  results = NULL
  results$method = method
  # Computing relevant statistics
  results$mean = mean(data$recovtime, na.rm = na.rm)
  results$se = sd(data$recovtime, na.rm = na.rm) / sqrt(sum(R_func(data)))
  results$cor_bp = cor(data$recovtime, data$bloodp, use = 'complete.obs',
                            method = 'pearson')
  results$cor_dose = cor(data$recovtime, data$logdose, use = 'complete.obs',
                             method = 'pearson') 
  return(results)
}
```

```{r 4a}
stats = data.frame(matrix(ncol = 5, nrow = 0))
cca_stats = get_statistics(databp, method = "CCA")
stats = rbind(stats, cca_stats)
stats
```

## Question (4b)
* **Description:** The same as in (a) but using mean imputation.

* **Answer:**
```{r 4b}
# Copy the dataset
mi_data = databp
# Mean imputation for recovery time
mi_data[!R, 3] = cca_stats$mean
# Store and report the statistics of the imputed data
mi_stats = get_statistics(mi_data, method = "MI")
stats = rbind(stats, mi_stats)
stats
```

## Question (4c)
* **Description:** The same as in (b) but using mean regression imputation.

* **Answer:**
```{r 4c}
# Copy the dataset
ri_data = databp
# Fit linear model for recovery time with blood pressure and log-dose as covariates
ri_fits = lm(recovtime ~ logdose + bloodp, data = ri_data)
# Impute missing recovery time values using model
ri_data[!R, 3] = predict(ri_fits, ri_data[!R, 1:2])
# Store and report the statistics of the imputed data
ri_stats = get_statistics(ri_data, method = "RI")
stats = rbind(stats, ri_stats)
stats
```

## Linear Model Check
We also check that our linear model fit on Y2 with Y1 is valid before moving on.
The final regression formula is observed to be $recovtime = 15.2158 + 11.4290*logdose - 0.2769*bloodp$ 
but all of these coefficients are considered to be insignificant for $\alpha = 0.05$.
This indicates a linear model is not sufficient in this context.

The residual and Q-Q plots are also unconvincing further reinforcing the argument
that our linear model is not sufficient for modelling the recovery time
accurately.
```{r, include = TRUE, message = FALSE}
# Summary of model
summary(ri_fits)
# Regression plot
plot(ri_fits$fitted.values, residuals(ri_fits), xlab = "Fitted values", ylab = "Residuals")
# q-q plot
qqnorm(rstandard(ri_fits), xlim = c(-3, 3), ylim = c(-3, 3))
qqline(rstandard(ri_fits), col = 2)
```

## Question (4d)
* **Description:** The same as in (c) but using stochastic regression imputation.

* **Answer:**
```{r 4d}
set.seed(1)
# Copy the dataset
sri_data = databp
# Fit linear model for recovery time with blood pressure and log-dose as covariates
sri_fits = lm(recovtime ~ logdose + bloodp, data = sri_data)
# Noise for stochastic regression imputation
noise = rnorm(n = sum(!R), mean = 0, sd = sigma(sri_fits))
# Impute missing recovery time values using model
sri_data[!R, 3] = predict(sri_fits, sri_data[!R, 1:2]) + noise
# Store and report the statistics of the imputed data
sri_stats = get_statistics(sri_data, method = "SRI")
stats = rbind(stats, sri_stats)
stats
```

## Question (4e)
* **Description:** You will now conduct the same analysis but applying another technique called predictive
mean matching (Little, 1988), which is a special type of hot deck imputation. In the
simplest form of this method (and the one you will use here), a regression model is used
to predict the variables with missing values from the other (complete) variables. For
each subject with a missing value, the donor is chosen to be the subject with a predicted
value of her or his own that is closest (to be measured by the squared difference) to the
prediction for the subject with the missing value.

* **Answer:**
```{r 4e}
set.seed(1)
# Copy the dataset
reg_data = databp
# Recycling sri_fits to get prediction for every row
reg_data[, 3] = predict(sri_fits, reg_data[, 1:2]) 
# Empty list to store predictions
preds = NULL

# ----------------- PREDICTIVE MEAN MATCHING ALGORITHM  ----------------- #
# Loop over missing subjects
for (i_miss in which(!R)) {
  # Predictions of recovery time for subjects with missing values
  recov_time = reg_data[i_miss, 3]
  
  # Store distances
  distances = vector(, nrow(reg_data))
  
  # Preventing the use of donors who originally has missing values
  distances[!R] = Inf
  
  # Compute distances for observed values
  distances[R] = (rep(recov_time, sum(R)) - reg_data[R, 3])^2
  
  # Get donor who has smallest distance
  donor = which.min(distances)
  cat(sprintf('Donor for subject %s is %s\n', i_miss, donor))
  
  # Store the original value of the recovery time from donor
  preds = c(preds, reg_data[donor, 3])
}
# Copy data for predictive mean matching 
pmm_data = databp
# Overwrite NAs with predictions from above
pmm_data[!R, 3] = preds
# Store and report the statistics of the imputed data
pmm_stats = get_statistics(pmm_data, na.rm = FALSE, method = "PMM")
stats = rbind(stats, pmm_stats)
stats
```

With all our single imputation methods now performed we can begin our comparative
analysis. We note that the standard error on the mean for all these single imputation 
methods is too low. This is because the imputed values are implicitly treated as
part of the complete dataset. We therefore overexaggerate our confidence in our
estimates of the mean which corresponds to a lower standard error to be observed
than what is appropriate. The additional uncertainty created due to sampling 
the complete data is completely ignored using these single imputation methods.

First comparing complete case analysis (CCA) and mean imputation (MI), 
we see both estimates produce the same mean but the standard error for MI is 
significantly lower due to the reason outlined previously. This notion pertains
its veracity for all the following single imputation methods discussed. It also interesting
to note the attenuation in the `cor_dose` variable. This is due to the fact that
when MI is performed, all the imputed values lie on a horizontal line hence reducing
the correlation between recovery time and the dosage of the drug administered.
This effect is damped for the case of blood pressure and recovery time since there 
doesn't seem to be a correlation between these two variables.

The converse effect is observed for regression imputation (RI) as now
all our imputed value lie on the regression line we constructed with our linear
model which will artificially increase correlations between the variate and 
covariates. Another downfall of this method is that it is very unrealistic since
it is highly unlikely that all our missing values would lie upon the same regression line.
RI therefore also reduces the realistic variability of the data.

We attempt to remedy this effect in the hope to emulate the variability of the 
real world by adding some noise to our regression line.
This method is known as stochastic regression imputation (SRI) explains the decrease
of `cor_dose` from 0.2801835 (RI) to 0.2566208 (SRI)

Finally, the results presented by predictive mean matching (PMM) are indicative
of a method which is a combination of the previous single imputation methods mentioned.

Overall, SRI and PMM are the most robust single imputation methods used in this 
analysis however it is not clear which methods imputations provide a dataset
that is closest to the true underlying distribution. All we can say is that
SRI and PMM will be more robust in a larger number of scenarios than CCA,
MI and RI.

# Question (4f)
* **Description:** What is an advantage of predictive mean matching over stochastic 
regression imputation? Based on your analysis, can you foresee any potential 
problem of predictive mean matching?

* **Answer:** An advantage of predictive mean matching over stochastic regression
imputation is that the imputed value that is used to replace the missing value 
originates from a subject who is *similar* (this is quantified using the 
squared difference). However, predictive mean matching would begin to produce
very biased estimates, in a similar manner to mean imputation, if the same donor
value is used to impute many missing values. This is a plausible situation when 
the sample size is small.