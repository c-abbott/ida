---
title: |
  <center> Incomplete Data Analysis </center>
  <center> Assignment 2 </center>
author: "<center> Callum Abbott </center>"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---
\fontsize{10}{20}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
All code used for this assignment can be found in the following repository  [https://github.com/c-abbott/ida] under the folder `assignment-2`.

## Question 1
Suppose $Y_1, . . . , Y_n$ are independent and identically distributed with cumulative 
distribution function given by
\begin{equation*}
F(y;\theta) = 1 - e^{-y^2/(2\theta)}, \quad y \geq 0, \; \theta > 0
\end{equation*}
Further suppose that observations are (right) censored if $Y_i > C$, for some known $C > 0$,
and let
\begin{equation*}
X_i=
\begin{cases}
Y_i \quad if \; Y_i \leq C,\\
C \quad if \; Y_i > C, 
\end{cases}
\quad
R_i=
\begin{cases}
1 \quad if \; Y_i \leq C\\
0 \quad if \; Y_i > C 
\end{cases}
\end{equation*}

### Question 1a
Show that the maximum likelihood estimator based on the observed data
$\{x_i,r_i\}_{i=1}^n$ is given by
\begin{equation*}
\hat{\theta} = \frac{\sum_{i=1}^n X_i^2}{2\sum_{i=1}^nR_i}
\end{equation*}

**Solution:**

* To derive the MLE we must maximize the log-likelihood of the observed data $\{(x_i,r_i)\}_{i=1}^n$.
In this context, there are two contributions to the likelihood function:

  1. $f(y_i;\theta) = \frac{dF(y_i;\theta)}{dy_i}$ from *non-censored* observations.

  2. $Pr(Y_i > C ; \theta) = S(C;\theta) = 1 -F(y_i;\theta)$ from *censored* observations.
  
* All observations $Y_i, ..., Y_n$ are iid, hence,

\vspace{-10mm}
\begin{align*}
L(\theta)&=\prod_{i=1}^{n}\left\{[f(y_i;\theta)]^{r_i}[S(C;\theta)]^{1-r_i}\right\}\\
&=\prod_{i=1}^{n}\left\{\left[\frac{y_i}{\theta} e^{-y_i^2/2\theta}\right]^{r_i}\left[e^{-C^2/2\theta}\right]^{1-r_i}\right\}\\
&=\left(\frac{y_i}{2\theta}\right)^{\sum_ir_i}exp\left(-\frac{1}{2\theta}\sum_i[r_iy_i^2 + (1-r_i)C^2]\right)\\
&=\left(\frac{y_i}{2\theta}\right)^{\sum_ir_i}exp\left(-\frac{1}{2\theta}\sum_ix_i^2\right)
\end{align*}

* Note that in order to understand how one goes from line 3 to line 4 in the equation
defined above, we recall that we can write the variable $X_i$ as $X_i = Y_iR_i + C(1-R_i)\\$
and due to the binary nature of $R_i$:

\vspace{-12mm}
\begin{align*}
&\implies X_i^2 = Y_i^2R_i^2 + C^2(1-R_i)^2 + 2Y_iR_iC(1-R_i)\\
&\implies X_i^2 = Y_i^2R_i + C^2(1-R_i)
\end{align*}

* We can now define the log-likelihood to be

\vspace{-10mm}
\begin{align*}
\log L(\theta) := l(\theta) = \sum_{i=1}^nr_i\log\left(\frac{y_i}{2\theta}\right) - \frac{1}{2\theta}\sum_{i=1}^nx_i^2
\end{align*}

* Maximising this quantity through taking its derivative  

\vspace{-7.5mm}
\begin{equation*}
\frac{\text{d}}{\text{d}\theta}l(\theta)=-\frac{\sum_{i=1}^{n}r_i}{\theta} + \frac{\sum_{i=1}^{n}x_i^2}{2\theta^2}
\end{equation*}

* leading to

\vspace{-10mm}
\begin{equation*}
\widehat{\theta}_{\text{MLE}}=\frac{\sum_{i=1}^{n}X_i^2}{2\sum_{i=1}^{n} R_i}.
\end{equation*}

* Note that we have assumed here that $\widehat{\theta}_{\text{MLE}}$ is indeed a maximum
and have not computed the second derivative since our result matches the one given
in the question.

### Question 1b
Show that the expected Fisher information for the observed data likelihood
is

\begin{equation*}
I(\theta) = \frac{n}{\theta^2}(1 - e^{-C^2/2\theta})
\end{equation*}

**Note:** $\int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/2\theta} + 2\theta(1-e^{-C^2/2\theta})$,
where $f(y;\theta)$ is the density function corresponding to the cumulative distribution function
$F(y;\theta)$ defined above.

\vspace{5mm}
**Solution:**

* We first recall the general definition of the expected Fisher information to be

\vspace{-5mm}
\begin{equation*}
I(\theta) = -\mathbb{E}\left[\frac{d^2l(\theta)}{d\theta^2}\right]
\end{equation*}

* We now compute the second derivative of the log-likelihood and re-introduce the
variables $r_i$ and $y_i$ for $x_i$ which will allow us to take expectations more clearly.
This yields,

\begin{equation*}
I(\theta) = -\frac{\sum_i \mathbb{E}[R_i]}{\theta^2} + \frac{\sum_i \mathbb{E}[R_iY_i^2]}{\theta^3} + \frac{\sum_iC^2\mathbb{E}[(1-R_i)]}{\theta^3} 
\end{equation*}

\newpage

* Note that $R$ is a binary random variable and so

\vspace{-10mm}
\begin{align*}
\mathbb{E}[R]&=1\times\Pr(R=1)+0\times\Pr(R=0)\\
&=\Pr(R=1)\\
&=\Pr(Y\leq C)\\
&=F(C;\theta)\\
&=1-e^{-C^2/2\theta}.
\end{align*}

* And hence

\vspace{-10mm}
\begin{align*}
I(\theta) &= -\frac{\sum_i \mathbb{E}[R_i]}{\theta^2} + \frac{\sum_i \mathbb{E}[R_iY_i^2]}{\theta^3} + \frac{\sum_iC^2\mathbb{E}[(1-R_i)]}{\theta^3} \\
&= -\frac{n}{\theta^2}(1-e^{C^2/2\theta}) + \frac{n}{\theta^3}\left\{-C^2e^{-C^2/2\theta} + 2\theta(1-e^{-C^2/2\theta})\right\} + \frac{n}{\theta^3}e^{-C^2/2\theta}\\
&= \frac{n}{\theta^2}(1 - e^{-C^2/2\theta})
\end{align*}

* Where the note given in the question was used to calculate $\mathbb{E}[R_iY_i^2] = \int_0^Cy^2f(y;\theta)dy = -C^2e^{-C^2/2\theta} + 2\theta(1-e^{-C^2/2\theta})$

### Question 1c
Appealing to the asymptotic normality of the maximum likelihood estimator, 
provide a $95\%$ confidence interval for $\theta$.

**Solution:**

* We recall the asymptotic normality of the MLE as

\vspace{-5mm}
\begin{equation*}
\widehat{\theta}_{\text{MLE}} \sim N(\theta, I(\theta)^{-1})
\end{equation*}

* Therefore

\vspace{-5mm}
\begin{equation*}
\frac{\widehat{\theta}_{\text{MLE}}-\theta}{\sqrt{I(\theta)^{-1}}} \sim N(0, 1)
\end{equation*}

* Using the properties of the standard Gaussian distribution ($\alpha = 0.05$)

\vspace{-5mm}
\begin{align*}
Pr\left(z_{-\alpha/2} \leq \frac{\widehat{\theta}_{\text{MLE}}-\theta}{\sqrt{I(\theta)^{-1}}} \leq z_{\alpha/2}\right) = 1-\alpha = 0.95
\end{align*}


* The $95\%$ CI for $\theta$ is hence
$\left[\sqrt{I(\theta)^{-1}}z_{-\alpha/2} + \widehat{\theta}_{\text{MLE}}, \sqrt{I(\theta)^{-1}}z_{\alpha/2} + \widehat{\theta}_{\text{MLE}}\right]$
where $z_{\alpha/2} = 1.959964$, $z_{-\alpha/2} = -1.959964$,
and $\sqrt{I(\theta)^{-1}} = \theta/\sqrt{n(1 - e^{-C^2/2\theta})}$ 


```{r}
alpha = 0.05
z = qnorm(1-alpha/2)
```

## Question 2
Suppose that $Y_i \sim N(\mu, \sigma^2)$ are iid for $i=1,...,n$. Further suppose that now 
observations are (left) censored if $Y_i < D$, for some known $D$ and let

\begin{equation*}
X_i=
\begin{cases}
Y_i \quad if \; Y_i \geq D,\\
D \quad if \; Y_i < D, 
\end{cases}
\quad
R_i=
\begin{cases}
1 \quad if \; Y_i \geq D\\
0 \quad if \; Y_i < D
\end{cases}
\end{equation*}

### Question 2a
Show that the log-likelihood of the observed data
$\{x_i,r_i\}_{i=1}^n$ is given by

\vspace{-5mm} 
\begin{equation*}
l(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r}) = \sum_{i=1}^n\left\{r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu, \sigma^2)\right\}
\end{equation*}

where $\phi(x_i;\mu,\sigma^2)$ and $\Phi(x_i;\mu, \sigma^2)$ stands, respectively, for the density function
and cumulative distribution function of the normal distribution with mean $\mu$ and variance $\sigma^2$.

**Solution:**

* Similar to 1a, our likelihood function has two contributions:

  1. $\phi(x_i;\mu, \sigma^2)$ from *non-censored* observations.

  2. $Pr(X_i < D ; \mu, \sigma^2) = S(D;\mu, \sigma^2) = \Phi(x_i;\mu, \sigma^2)$ from *censored* observations.
  
* All observations $X_i, ..., X_n$ are iid, hence,

\vspace{-10mm}
\begin{align*}
l(\mu, \sigma^2|\boldsymbol{x}, \boldsymbol{r})&=\log \prod_{i=1}^{n}\left\{\phi(x_i;\mu,\sigma^2)]^{r_i}[\Phi(x_i;\mu, \sigma^2)]^{1-r_i}\right\}\\
&= \log \left\{\phi(x_i;\mu,\sigma^2)]^{\sum_i r_i}[\Phi(x_i;\mu, \sigma^2)]^{\sum_i(1-r_i)}\right\}\\
&=  \sum_{i=1}^n\left\{r_i\log\phi(x_i; \mu, \sigma^2) + (1-r_i)\log\Phi(x_i;\mu, \sigma^2)\right\}
\end{align*}


### Question 2b
Determine the maximum likelihood estimate of $\mu$ based on the data available
in the file dataex2.Rdata. Consider $\sigma^2$ known and equal to $1.5^2$.

**Solution:**

* Given that we are now in possession of the log-likelihood of our data, we proceed
to maximize this with respect to the parameter $\mu$ in order to derive our MLE
of this parameter which we denote $\widehat{\mu}_{\text{MLE}}$.

* The Newton-Raphson method was used to optimise our log-likelihood which yielded the following MLE: $\widehat{\mu}_{\text{MLE}} = 5.5328$ to 4 d.p.

```{r ex2, message=FALSE, warning=FALSE}
library(maxLik)
# Loading in data
load('dataex2.Rdata')

# Log likelihood function set to maximized
get_log_likelihood = function(param, data) {
  mu = param
  x = data[,1]; r = data[,2]
  return(sum(r*dnorm(x, mean=mu, sd=1.5, log=TRUE) + 
               (1 - r)*pnorm(x, mean=mu, sd=1.5, log.p=TRUE)))
}
# Get MLE
mle = maxLik(logLik = get_log_likelihood, data = dataex2, start = c(mu=1))
# Present results
summary(mle)
```

\newpage

## Question 3
Consider a bivariate normal sample $(Y_1, Y_2)$ with parameters $\theta=(\mu_1,\mu_2,\sigma_1^2,\sigma_{12},\sigma_2^2)$ The
variable $Y_1$ is fully observed, while some values of $Y_2$ are missing. Let $R$ be the missingness
indicator, taking the value 1 for observed values and 0 for missing values. For the following
missing data mechanisms state, justifying, whether they are ignorable for likelihood-based
estimation.

**Solution:** 

* A missing data mechanism (MDM) is said to be ignorable for likelihood based inference if 
and only if the following two criteria are met:

  1. The missing data are missing at random (MAR) or missing completely at random (MCAR).
  
  2. The parameter $\psi$ (missingness mechanism) and $\theta$ (data model) are distinct
  in the sense that the joint parameter space of $(\psi, \theta)$ is the product of the
  parameter spaces $\Psi$ and $\Theta$ (separability condition).
  

(a) $logit \left\{Pr(R=0|y_1,y_2,\theta,\psi)\right\} = \psi_0 + \psi_1y_1; \quad \psi = (\psi_1,\psi_2)$ distinct from $\theta$.

*  We observe that the MDM is dependent on the fully observed variable, $y_1$, only.
The missing data resulting from this mechanism will hence be MAR. Furthermore, $\psi$ (missingness mechanism) and $\theta$ (data model) are distinct indicating that MDM (a) is ignorable for likelihood based
inference.

(b) $logit \left\{Pr(R=0|y_1,y_2,\theta,\psi)\right\} = \psi_0 + \psi_1y_2; \quad \psi = (\psi_1,\psi_2)$ distinct from $\theta$.

* We observe that the MDM is dependent on the missing variable, $y_2$, only.
The missing data resulting from this mechanism will hence be missing not at random (MNAR) indicating that MDM (b) is **NOT** ignorable for likelihood-based inference.

(c) $logit \left\{Pr(R=0|y_1,y_2,\theta,\psi)\right\} = 0.5(\mu_1 + \psi_1y_1); \quad  \psi$ (scalar) distinct from $\theta$.

* We observe a similar MDM to (a) with an added dependency on $\mu_1$. The parameter 
$\mu_1$ is contained within the parameter space of both the missing data model, $\theta$, and 
the MDM, $\psi$. Criterion 2 is hence not met and MDM (c) is **NOT** ignorable for
likelihood-based inference

\newpage

## Question 4
Suppose that

\vspace{-10mm}
\begin{align*}
Y_i &\overset{\text{ind}}{\sim} \text{Bernoulli}(p_i(\boldsymbol{\beta}),\\
p_i(\boldsymbol{\beta}) &= \frac{exp(\beta_0 + x_i\beta_1)}{1+exp(\beta_0+x_i\beta_1)}
\end{align*}

for $i=1,...,n$ and $\boldsymbol{\beta} = (\beta_0, \beta_1)'$. Although the covariate $x$
is fully observed, the response variable $Y$ has missing values. Assuming ignorability, derive
and implement the EM algorithm to compute the MLE of $\boldsymbol{\beta}$ based on the data
available in `dataex4.Rdata`. 

**Solution:**

* We begin by first obtaining the likelihood for $\boldsymbol{\beta}$ which
is given by

\vspace{-10mm}
\begin{align*}
L(\boldsymbol{\beta})&=\prod_{i=1}^{n}\{p_i(\boldsymbol{\beta})^{y_i}[1-p_i(\boldsymbol{\beta})]^{1-y_i}\}\\
&=\prod_{i=1}^{n}\left\{\left(\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\right)^{y_i}\left(\frac{1}{1+e^{\beta_0+x_i\beta_1} }\right)^{1-y_i}\right\}.
\end{align*}

* The corresponding log likelihood is hence

\vspace{-7.5mm}
\begin{align*}
l(\boldsymbol{\beta})&=\sum_{i=1}^{n}\left\{y_i\log\left(\frac{e^{\beta_0+x_i\beta_1}}{1+e^{\beta_0+x_i\beta_1}}\right)+(1-y_i)\log\left( \frac{1}{1+e^{\beta_0+x_i\beta_1}}\right)\right\}\\
&=\sum_{i=1}^{n}\{y_i(\beta_0+x_i\beta_1)-\log(1+e^{\beta_0+x_i\beta_1})\}.
\end{align*}

* Now that we are in possession of the log-likelihood, we can use this to conduct the
expectation step of the EM algorithm and define our $Q$ function.

* Note that the expectation is taken under the distribution of the **missing data**. We 
hence make use of our univariate pattern of missingness and assume that the first
$m$ values of $Y$ are reserved and the remaining $n-m$ are missing i.e.
$\boldsymbol{y_{obs}} = y_1,...,y_m$ and $\boldsymbol{y_{mis}} = y_m+1,...,y_n$.

\vspace{-10mm}
\begin{align*}
  Q(\boldsymbol{\beta}|\boldsymbol{\beta^{(t)}}) &= \mathbb{E}_{\boldsymbol{y_{mis}}}\left[l(\boldsymbol{\beta})|\boldsymbol{y_{obs}}, \boldsymbol{x}, \boldsymbol{\beta^{(t)}}\right]\\
  &=\sum_{i=1}^m\left\{y_i(\beta_0 + \beta_1x_i)\right\} - \sum_{i=1}^n\log(1+e^{\beta_0 + \beta_1x_i}) + 
  \sum_{i=m+1}^n(\beta_0 + \beta_1x_i) \mathbb{E}_{\boldsymbol{y_{mis}}}[y_i|\boldsymbol{y_{obs}}, \boldsymbol{x}, \boldsymbol{\beta^{(t)}}]\\
  &=\sum_{i=1}^m\left\{y_i(\beta_0 + \beta_1x_i)\right\} - \sum_{i=1}^n\log(1+e^{\beta_0 + \beta_1x_i}) + 
  \sum_{i=m+1}^n(\beta_0 + \beta_1x_i)p_i(\boldsymbol{\beta})
\end{align*}

* Where we have used the result that $\mathbb{E}[Y_i] = p_i(\boldsymbol{\beta})$ since $Y_i \overset{\text{ind}}{\sim} \text{Bernoulli}\{(p_i(\boldsymbol{\beta})\}$. We remind the reader of the definition of  $p_i(\boldsymbol{\beta})$
is $p_i(\boldsymbol{\beta}) = \frac{exp(\beta_0 + x_i\beta_1)}{1+exp(\beta_0+x_i\beta_1)}$ as defined in the question.

* Following our definition of the Q function, we conduct the maximization step of the EM algorithm
and maximize this function with respect to the parameters, $\boldsymbol{\beta} = (\beta_0, \beta_1)'$. 

* The R code below repeatedly maximizes our Q function for every iteration of our
algorithm in order to find subsequent values of the parameters considered.

* This process is repeated until the following convergence criterion is met and we have
our MLEs ($\epsilon = 1 \times 10^{-10}$):

\vspace{-5mm}
\begin{equation*}
  |p^{(t+1)} - p^{(t)}|+|\mu^{(t+1)}-\mu^{(t)}|+|(\sigma^{(t+1)})^2 - (\sigma^{(t)})^2|+|\lambda^{(t+1)} - \lambda^{(t)}| < \epsilon
\end{equation*}

* At convergence, the following results are achieved for our MLEs: $\widehat{\beta_0}_{\text{MLE}} =  0.9755$ to 4 d.p and $\widehat{\beta_1}_{\text{MLE}} =  -2.4804$ to 4 d.p as our MLEs for $\boldsymbol{\beta} = (\beta_0, \beta_1)'$.


```{r ex4, message=FALSE, warning=FALSE}
# Loading relevant packages
library(maxLik)
library(dplyr)
library(tidyr)
library(magrittr)

# Loading data
load('dataex4.Rdata')

dataex4 = dataex4 %>%
  # Sorting data in ascending order in column Y (0 -> 1 -> NA)
  arrange(Y) %>%
  # Creating indicator variable column (if Y == NA -> R = 0, else -> R = 1)
  mutate(R = (Y == 0 | Y == 1)*1) %>%
  # Replacing NAs with 0s in R column
  tidyr::replace_na(list(R = 0)) %>%
  # Replacing NAs with 2s in Y column to prevent coercion problems i.e. 0*NA=NA
  tidyr::replace_na((list(Y = 2)))

# Sigmoid probability function  
prob = function(beta, x) {
  return(exp(beta[1] + x*beta[2]) / (1 + exp(beta[1] + x*beta[2])))
}

# Defining Q function for EM algorithm
q_function = function(params, data){
  beta0 = params[1]; beta1 = params[2]
  xx = data$X
  yy = data$Y
  rr = data$R
  sum(yy*rr*(beta0 + beta1*xx) - log(1 + exp(beta0 + beta1*xx)) + 
        (1 - rr)*(beta0 + beta1*xx)*prob(beta.old, xx))
}

# Repeatedly maximising to get \beta^(t+1)
# until convergence criterion is met
tol = 1e-10
beta.old = c(0, 0)
repeat{
  beta = coef(maxLik(q_function, data=dataex4, start = beta.old))
  if (max(abs(beta - beta.old)) < tol) {
    break
  }
  beta.old = beta
}
beta
```


\newpage

## Question 5
Consider a random sample $Y_1,...,Y_n$ from the mixture distribution with density 
 
\begin{equation*}
  f(y) = pf_{\text{logNormal}}(y;\mu, \sigma^2) + (1-p)f_{\text{Exp}}(y;\lambda),
\end{equation*}

with

\vspace{-10mm}
\begin{align*}
  f_{\text{logNormal}}(y;\mu, \sigma^2) &= \frac{1}{y\sqrt{2\pi\sigma^2}}\text{exp}\left\{\frac{1}{2\sigma^2}(\log y - \mu)^2\right\}, 
  \quad y>0, \; \mu \in \mathbb{R}, \; \sigma > 0 \\
  f_{\text{Exp}}(y;\lambda) &= \lambda e^{-\lambda y}, \quad y \geq 0, \quad \lambda > 0
\end{align*}

and $\boldsymbol{\theta} = (p, \mu, \sigma^2, \lambda)$

### Question 5a

Derive the EM algorithm to find the updating equations for $\boldsymbol{\theta^{(t+1)}} = (p^{(t+1)}, \mu^{(t+1)}, (\sigma^{(t+1)})^2, \lambda^{(t+1)})$.

**Solution:**

* We begin by taking our mixture model and computing the likelihood:

\vspace{-5mm}
\begin{equation*}
  L(\boldsymbol{\theta}; y) = \prod_{i=1}^n\left\{pf_{\text{logNormal}}(y_i;\mu, \sigma^2) + (1-p)f_{\text{Exp}}(y_i;\lambda)\right\}
\end{equation*}

* The log likelihood is hence

\vspace{-5mm}
\begin{equation*}
  l(\boldsymbol{\theta}; y) = \sum_{i=1}^n\log\left\{pf_{\text{logNormal}}(y_i;\mu, \sigma^2) + (1-p)f_{\text{Exp}}(y_i;\lambda)\right\}
\end{equation*}

* The combination of the summation and logarithm make it very difficult to maximize this log-likelihood. We hence turn to the EM algorithm for this computation, but first, we must artificially create *missing data* in order to implement it.

* The idea that enables us to implement the EM algorithm is that if we knew the group the observation $y_i$ belonged to, then we could fit the appropriate distribution (log-normal or exponential).

* Let us define $\boldsymbol{y_{\text{obs}}} = (y_1,...,y_n)$ and $\boldsymbol{y_{\text{mis}}} = \boldsymbol{z} = (z_1,...,z_n)$ where $Z_i \sim \text{Bernoulli}(p)$ and hence

\begin{equation*}
Z_i=
\begin{cases}
1 \quad \text{if} \; Y_i \; \text{belongs to} \; f_{\text{logNormal}}(y_i;\mu, \sigma^2)  \\
0 \quad \text{if} \; Y_i \; \text{belongs to} \; f_{\text{Exp}}(y_i;\lambda)
\end{cases}
\end{equation*}

* Re-writing our log-likelihood in terms of $\boldsymbol{z}$

\vspace{-5mm}
\begin{equation*}
  l(\boldsymbol{\theta}; y, \boldsymbol{z}) = \sum_{i=1}^nz_i\left\{\log p + \log f_{\text{logNormal}}(y_i;\mu, \sigma^2)\right\} +  \sum_{i=1}^n(1-z_i)\left\{\log (1-p) + \log f_{\text{Exp}}(y_i;\lambda)\right\}
\end{equation*}

* With our log-likelihood and artificial missing data in hand we can now define our Q function to be:

\vspace{-10mm}
\begin{align*}
Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}}) &=
\mathbb{E}_{\boldsymbol{z}}\left[l(\boldsymbol{\theta}; y, \boldsymbol{z})\right]\\
&= \sum_{i=1}^n\mathbb{E}[\boldsymbol{z}|\boldsymbol{y}, \boldsymbol{\theta}^{(t)}]\left\{\log p + \log f_{\text{logNormal}}(y_i;\mu, \sigma^2)\right\} +  \sum_{i=1}^n(1-\mathbb{E}[\boldsymbol{z}|\boldsymbol{y}, \boldsymbol{\theta}^{(t)}])\left\{\log (1-p) + \log f_{\text{Exp}}(y_i;\lambda)\right\}
\end{align*}

* Taking the relevant expectation

\vspace{-5mm}
\begin{align*}
\mathbb{E}[\boldsymbol{z}|\boldsymbol{y},\boldsymbol{\theta}^{(t)}] &= \mathbb{E}[z_i|\boldsymbol{y},\boldsymbol{\theta}^{(t)}]\\ 
  &= \text{Pr}(z_i=1|y_i,\boldsymbol{\theta}^{(t)})\\
  &=\frac{p_i^{(t)}f_{\text{logNormal}}(y_;\mu, \sigma^2)}{p_i^{(t)}f_{\text{logNormal}}(y_;\mu, \sigma^2) + (1-p_i^{(t)})f_{\text{Exp}}(y_i;\lambda)}\\ &:= \tilde{p}_i^{(t)}
\end{align*}

* We hence define our Q function to be

\vspace{-5mm}
\begin{equation*}
Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}}) 
= \sum_{i=1}^n \tilde{p}_i^{(t)}\left\{\log p + \log f_{\text{logNormal}}(y_i;\mu, \sigma^2)\right\} +  \sum_{i=1}^n(1- \tilde{p}_i^{(t)})\left\{\log (1-p) + \log f_{\text{Exp}}(y_i;\lambda)\right\}
\end{equation*}

* With the Q function defined above, we can analytically maximize this function in order to find $\boldsymbol{\theta}^{(t+1)}$. Taking the derviative of the Q function with respect to all our parameters yields

\vspace{-10mm}
\begin{align*}
\frac{\partial Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})}{\partial p} &= \frac{1}{p}\sum_{i=1}^n\tilde{p}_i^{(t)} - \frac{1}{(1-p)}\sum_{i=1}^n(1-\tilde{p}_i^{(t)})\\
\frac{\partial Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})}{\partial \mu} &= \sum_{i=1}^n\tilde{p}_i^{(t)}\left\{\frac{\log y_i - \mu}{\sigma^2}\right\}\\
\frac{\partial Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})}{\partial \sigma^2} &=
\sum_{i=1}^n\tilde{p}_i^{(t)}\left\{\frac{(\log y_i - \mu)^2}{2\sigma^4} - \frac{1}{2\sigma^2}\right\}\\
\frac{\partial Q(\boldsymbol{\theta}|\boldsymbol{\theta^{(t)}})}{\partial \lambda} &=
\sum_{i=1}^n(1-\tilde{p}_i^{(t)})\left\{\frac{1}{\lambda} - y_i\right\}
\end{align*}

* Setting the each derivative defined above to zero and solving for each parameter in $\boldsymbol{\theta}$ at iteration $t+1$ we yield the following update equations

\vspace{-10mm}
\begin{align*}
p^{(t+1)} &= \frac{1}{n}\sum_{i=1}^n\tilde{p}_i^{(t)}\\
\mu^{(t+1)} &= \frac{\sum_{i=1}^n\tilde{p}_i^{(t)}\log y_i}{\sum_{i=1}^n\tilde{p}_i^{(t)}}\\
\left(\sigma^{(t+1)}\right)^2 &= \frac{\sum_{i=1}^n\tilde{p}_i^{(t)}(\log y_i - \mu^{(t+1)})^2}{\sum_{i=1}^n\tilde{p}_i^{(t)}}\\
\lambda^{(t+1)} &= \frac{\sum_{i=1}^n(1-\tilde{p}_i^{(t)})}{\sum_{i=1}^n(1-\tilde{p}_i^{(t)})y_i}
\end{align*}


### Question 5b

Using the dataset `datasetex5.Rdata` implement the EM algorithm and find the MLEs
for each component of $\theta$. As starting values, you might want to consider
$\theta^{(0)} = (p^{(0)}, \mu^{0)}, (\sigma^{(0)})^2, \lambda^{(0)}) = (0.1, 1, 0.5^2, 2)$.
Draw the histogram of the data with the estimated density superimposed.

**Solution:**

* Using the code defined below (see comments for details on implementation), we find the MLEs for each component of $\boldsymbol{\theta}$ to be (4 d.p):
$\widehat{p}_{\text{MLE}} = 0.4795$;
$\widehat{\mu}_{\text{MLE}} = 2.0133$; 
$\widehat{\sigma^2}_{\text{MLE}} = 0.8637$; 
$\widehat{\lambda}_{\text{MLE}} = 1.0330$

```{r ex5}
load('dataex5.Rdata')

# Calculates tilde p for all i, with the current parameter values
p_tilde <- function(y, p.t, mu.t, sigma2.t, lambda.t) {
  c1 <- p.t * dlnorm(y, mu.t, sqrt(sigma2.t))
  c2 <- (1 - p.t) * dexp(y, lambda.t)
  c1 / (c1 + c2)
}

# Applies the EM algorithm to fit the mixture density with provided observations y
em <- function(y, p.t, mu.t, sigma2.t, lambda.t, eps = 1e-8, maxit = 1e3) {
  n <- length(y)
  t <- 0
  
  # Until converged or maximum iterations reached, iterate indefinitely...
  repeat {
    # Calculate the tilde p values for all i, using the current parameter values
    p_tilde.t <- p_tilde(y, p.t, mu.t, sigma2.t, lambda.t)
    # Store tilde p sum for efficiency, since this is used in multiple update equations
    sum.p_tilde.t <- sum(p_tilde.t)
    
    # Store the previous parameter values (used later for convergence check)
    params.prev <- c(p.t, mu.t, sigma2.t, lambda.t)
    
    # Update parameters using the update equations
    p.t <- sum.p_tilde.t / n
    mu.t <- sum(p_tilde.t * log(y)) / sum.p_tilde.t
    sigma2.t <- sum(p_tilde.t * (log(y) - mu.t)^2) / sum.p_tilde.t
    lambda.t <- (n - sum.p_tilde.t) / sum((1 - p_tilde.t) * y)
    
    # Increment iteration counter
    t <- t + 1
    
    # Store all parameters into an output variable
    output <- list(p = p.t, mu = mu.t, sigma2 = sigma2.t, lambda = lambda.t)
    attributes(output)$iter <- t
    
    if (t == maxit) {
      warning('Reached maximum number of iterations')
      return(output)
    } else if (sum(abs(params.prev - c(p.t, mu.t, sigma2.t, lambda.t))) < eps) {
      # Convergence check: sum|theta^(t) - theta^(t+1)| < epsilon (over i)
      return(output)
    }
  }
}
# Apply the EM algorithm to dataex5 to find the MLEs
theta <- em(dataex5, p.t = 0.1, mu.t = 1, sigma2.t = 0.25, lambda.t = 2)
theta
```


* We will now draw the histogram of the data with estimated density superimposed:

```{r ex5hist, message = FALSE, warning = FALSE}
load('dataex5.Rdata')

# Get params from MLE theta
p = theta[[1]]; mu = theta[[2]]; sigma2 = theta[[3]]; lambda = theta[[4]]

# Histogram of data with estimated density superimposed
hist(dataex5, breaks = 35, main="Random Sample from Mixture Distribution",
  xlab = "y",
  cex.main = 1.5,
  col= "lightblue",
  ylim= c(0,0.20),
  freq = FALSE)
  # Superimposing mixed density distribution
  curve(p*dlnorm(x, meanlog = mu, sdlog= sqrt(sigma2)) + (1-p)*dexp(x, lambda),
  add= TRUE, col= "red",lwd=1.5)
  legend("topright", c("Histogram Data", "Mixed Distribution density"),
  fill=c("lightblue", "red"))
```

