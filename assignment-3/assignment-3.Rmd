---
title: |
  <center> Incomplete Data Analysis </center>
  <center> Assignment 2 </center>
author: "<center> Callum Abbott </center>"
output:
  pdf_document: default
  html_document: default
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE)
library(mice)
library(rjags)
```

All code used for this assignment can be found in the following repository [https://github.com/c-abbott/ida] under the folder `assignment-3`.

roses are \textcolor{red}{red}.

## Question 1
Consider the `nhanes` dataset in `mice`.

### Question 1a
*(a) What percentage of the cases are incomplete?*

**Solution:**
We observe the NHANES dataset has a total of $25$ cases, $12$ of which are
incomplete. This gives the incomplete case percentage as $48.0\%$.

```{r echo=TRUE}
# Load in dataset
data(nhanes)
# Calculate NA percentage
na_percentage = sum(!complete.cases(nhanes)) / nrow(nhanes) * 100
```

### Question 1b
*(b) Impute the data with `mice` using the defaults with `seed=1`, in step 2 predict*
*`bmi` from `age`, `hyp`, and `chl` by the normal linear regression model, and then pool the*
*results. What are the proportions of variance due to the missing data for each parameter?*
*Which parameters appear to be most affected by the non-response?*

**Solution:**

Let *the proportions of variance due to the missing data for each parameter* be denoted as $\lambda$. 

Mathematically, we specify $\lambda$ to be:
\begin{equation*}
  \lambda = \frac{B + \frac{B}{M}}{V^{\text{MI}}}
\end{equation*}

where $M$ denotes the number of imputed datasets by `mice` (default = 5), $B = \frac{1}{M-1}\sum_{m=1}^M\left(\hat{\theta}^{(m)}-\hat{\theta}^{MI}\right)^2$ denotes the *between-imputation* variance, and $V^{\text{MI}} = \bar{U} + (1 + 1/M)B$ is the *total variance*. Note that $\bar{U} = \frac{1}{M}\sum_{m=1}^M\hat{U}^{(m)}$ is known as the
*within-variance*.

Using the standard MICE procedure - `mice(), with(), pool()` - we yield the following
values for *the proportions of variance due to the missing data for each parameter* (3SF, seed=1):

* `age` = 0.686
* `hyp (positive)` = 0.350
* `chl` = 0.304

We observe that the variable `age` has the highest value of $\lambda$ indicating that
this variable is most affected by the non-response.

```{r}
imps = mice(nhanes, seed=1, printFlag=FALSE) # MI step
fits = with(imps, lm(bmi ~ age + hyp + chl)) # Fitting lm to predict BMI
bmi_ests = pool(fits) # Gather useful summary statistics
bmi_ests[,3][c(1, 3, 10)] # Extracting only the relevant statistics
```

### Question 1c 
*(c) Repeat the analysis for* `seed` $\in {2,3,4,5,6}$. *Do the conclusions remain the same?*

**Solution:**
Repeating an analogous analysis but now varying the seeds to the values given above we
yield different conclusions. For seeds 2, 3 and 6, the `age` variable had the largest
value of $\lambda$ with respective values of 0.403, 0.590 and 0.655.
Meanwhile the `chl` and `hyp` variables had the largest $\lambda$ values
for seeds 4 and 5 with values 0.331 and 0.594 respectively. Other numerical values have not been included for brevity purposes.

This behaviour can be explained by the fact that $48%$ of our data is missing in the
NHANES dataset. Therefore, as we adjust the random seed, the proportion of variance due to the missing data can change significantly since it occupies such a large proportion of our
total data and hence leads us to conflicting conclusions.

```{r}
bmi2 = pool(with(mice(nhanes, seed=2, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi3 = pool(with(mice(nhanes, seed=3, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi4 = pool(with(mice(nhanes, seed=4, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi5 = pool(with(mice(nhanes, seed=5, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi6 = pool(with(mice(nhanes, seed=6, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
```

### Question 1d
*(d) Repeat the analysis with M = 100 with the same seeds. Would you prefer*
*these analyses over those with M = 5? Explain why.*

**Solution:**
Given that the conclusions derived in 1(c) varied with the random seed, this indicates 
we must increase the value of $M$ (the number of imputed datasets) in order to reduce
the probability of our conclusions being governed by random chance.

We set $M=100$ and repeat the analysis outlined in 1(b) and 1(c). We now find that `age`
is the parameter most affected by the non-response (largest $\lambda$) in 5 of the 6 seeds
with `chl` taking the same stance for seed 3.

Overall, I would favour the conclusions derived from the $M=100$ results since
if we analyze the expression of the *total variance* of the imputed parameter estimates
(in this case `bmi`),

\begin{equation*}
  V^{\text{MI}} = \bar{U} + B + \frac{B}{M}
\end{equation*}

where $\bar{U}$ is the *within-imputation variance* and $B$ is again the 
*between-imputation variance*, we see that increasing $M$ has the effect of reducing
this quantity thus increasing the reliability of estimations. It should be noted that both
$B$ and $\bar{U}$ are also  $\propto \frac{1}{M}$ which further reinforces this notion. 

On the contrary, a larger $M$ results in a lower level of statistical efficiency i.e.
one should be able to arrive at the same statistically significant conclusions
using a lesser amount of computational resources and time. In this context, $M=100$ is
easily handled by most modern machines given the NHANES dataset contains only 25 cases - a low
burden compared to standards of modern datasets.

```{r}
#mbmi1 = pool(with(mice(nhanes, seed=1, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi2 = pool(with(mice(nhanes, seed=2, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi3 = pool(with(mice(nhanes, seed=3, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi4 = pool(with(mice(nhanes, seed=4, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi5 = pool(with(mice(nhanes, seed=5, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi6 = pool(with(mice(nhanes, seed=6, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
```

# Question 2
*Each of the 100 datasets contained in `dataex2.Rdata` was generated in the* 
*following way*
\begin{equation*}
 y_i | x_i \stackrel{\text{ind.}}{\sim} \mathcal{N}(\beta_0+\beta_1x_i,1), \quad x_i \stackrel{\text{iid}}{\sim} \text{Unif}(-1, 1), \quad \beta_0 = 1, \quad \beta_1 = 3
\end{equation*}
*for* $i = 1,...,100$. *Additionally, some of the responses were set to be missing using a*
*MAR mechanism.* 

*The goal of this exercise is to study the effect that acknowledging/not*
*acknowledging parameter uncertainty when performing step 1 of multiple imputation might*
*have on the coverage of the corresponding confidence intervals. Further suppose that the*
*analysis of interest in step 2 is to fit the regression model that was used to generate the data,*
*i.e., a normal linear regression model where the response is* $y$ *and the covariate is* $x$.

*With the*
*aid of the mice package, calculate the empirical coverage probability of the* $95\%$ *confidence*
*intervals for* $\beta_1$ *under the following two approaches: stochastic regression imputation and*
*the corresponding bootstrap based version. Comment. For both approaches, please consider*
$M = 20$ *and* $\text{seed}=1$

**Solution:**
The R code below compares two methods used to impute missing values in incomplete datasets
through comparing empirical confidence intervals for the $\beta_1$ parameter -
these methods are stochastic regression imputation (SRI) and bootstrap sampling.

We remind the reader how confidence intervals (CIs) of an unknown parameter are interpreted 
in the frequentist paradigm of statistics. Upon collecting a sample of data, 
a $\text{p}\%$ CI can be constructed from this data. If such intervals were constructed
repeatedly from collecting more samples of the data, the $\text{p}\%$ CI will contain
the unknown parameter $\text{p}\%$ of the time.

The R code below hence performs a bootstrap and SRI imputation
method for each of the 100 datasets contained withing `dataex2.Rdata`. Once the
missing data has been imputed, we derive the $95\%$ CIs for each method and
check whether the true value of $\beta_1 = 3$ is contained within
the bootstrap, or SRI, $95\%$ CI. Given the CI interpretation outlined above, one
would expect the true value of $\beta_1$ to be contained within CIs $95\%$ of
the time for both imputation methods.

In reality, we observe this is true for the bootstrap method (`count_boot = 95`)
but not for the SRI method whose CIs only contain the true value of $\beta_1$ 
$88\%$ of the time (`count_sri = 88`). This is because, unlike the bootstrap imputation
method, SRI does not take into consideration the uncertainty surrounding the 
imputed values. More simply put, the SRI method treats the imputed values as if
they were truly observed rather than imputations from missing values.
This treatment is unjustified and hence results in the construction of more narrow, 
overly-confident CIs surrounding $\beta_1$ which are less likely to contain
the true value of $\beta_1$.

```{r}
load('dataex2.Rdata') # Loading in datasets

# Counters to track empirical coverage probability
count_sri = 0
count_boot = 0

# Imputations begin...
for (dataset in 1:2){
  # SRI & Bootstrap MICE methods initialized
  imps_sri = mice(dataex2[,,dataset], m=20, seed=1, printFlag=FALSE, method="norm.nob")
  imps_boot = mice(dataex2[,,dataset], m=20, seed=1, printFlag=FALSE, method="norm.boot")
  # Grab 95% CIs
  stats_sri = summary(pool(with(imps_sri, lm(Y ~ X))), conf.int=TRUE)
  stats_boot = summary(pool(with(imps_boot, lm(Y ~ X))), conf.int=TRUE)
  # Checking whether true value (3) of beta1 is in the CIs
  if (stats_sri$`2.5 %`[2] <= 3 & stats_sri$`97.5 %`[2] >= 3){
    count_sri = count_sri + 1
  }
  if (stats_boot$`2.5 %`[2] <= 3 & stats_boot$`97.5 %`[2] >= 3){
    count_boot = count_boot + 1
  } 
}
```


# Question 3
*Show that for a linear (in the coefficients) regression model, the following two*
*strategies coincide:*

* Method 1: *Computing the predicted values (point estimates) from each fitted model in step 2 and*
   *then pooling them according to Rubin’s rule for point estimates (i.e., averaging the*
    *predicted values across the imputed datasets).*
      
* Method 2: *Pooling the regression coefficients from each fitted model in step 2 using Rubin’s rule*
   *for point estimates and then computing the predicted values afterwards.*
       
**Solution:**

Let us begin by first defining a general linear (in the coefficients) regression
model with a total of $N$ covariates to act as our substantive model in the multiple
imputation process ($\epsilon \sim \mathcal{N}(0, \sigma^2)$).

\begin{align*}
  y &= \sum_{i=0}^N \beta_ix_i + \epsilon \\
  &= \beta_0 + \sum_{i=1}^N \beta_ix_i + \epsilon
\end{align*}

Let us assume that step 1 of the multiple imputation process has been performed
and we are in possession of a total of $M$ imputed datasets. We fit the previously
defined regression model to each of these M datasets yielding a set of predictions, $\mathbf{\hat{y}} = (\hat{y}^{(1)},\hat{y}^{(2)},...,\hat{y}^{(M)})$. Method 1 instructs us to pool these predictions according to Rubin's rule for point estimates yielding,

\begin{align*}
  \tilde{y} &= \frac{1}{M}\sum_{m=1}^M\color{green}{\hat{y}^{(m)}}\\
  &= \frac{1}{M}\sum_{m=1}^M\color{green}{\sum_{i=0}^N \beta_i^{(m)}x_i}\\
  &= \color{red}{\sum_{i=0}^Nx_i}\color{blue}{\frac{1}{M}\sum_{m=1}^M\beta_i^{(m)}}\\
  &= \color{red}{\sum_{i=0}^N}\color{blue}{\tilde{\beta}_i}\color{red}{x_i}
\end{align*}

where $\tilde{\beta}_i=\frac{1}{M}\sum_{m=1}^M\beta_i^{(m)}$ - **the pooled regression
coefficients according to Rubin's rules (Method 2)**.

The equations derived above hence indicate that Method 1 and Method 2 are mathematically
equivalent and will lead to the same results and conclusions.

# Question 4 
*The goal of this exercise is to study different ways of using* `mice` *when the analysis model*
*of interest/substantive model involves an interaction term between incomplete variables. The*
*model used to generate the data (available in* `dataex4.Rdata`), *which corresponds to our*
*model of interest in step 2, was the following one:*

\begin{align*}
  y_i &= \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3x_{1i}x_{2i} + \epsilon_i,\\
  x_{1i} &\stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1), \quad x_{2i} \stackrel{\text{iid}}{\sim} \mathcal{N}(1.5, 1), \quad \epsilon_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)
\end{align*}
*for* $i=1,...,1000, \beta_0=1.5, \beta_1=1, \beta_2=2, \beta_3=1$.

*Additionally, missingness was*
*imposed on* $y$ *and* $x_1$ *and so the interaction variable* $x_1x_2$ *also has missing values, although*
*the missingness in this interaction variable is induced by the missing in the covariate* $x_1$.

## Question 4a
*By only imputing the* $y$ *and* $x_1$ *variables in step 1, provide the estimates of*
$\beta_1, \beta_2\; \text{and} \; \beta_3$ *along with 95% confidence intervals. Comment. Note that this approach*
*where the interaction variable is left outside the imputation process and calculated afterwards in the analysis* *model, is known as Impute, then transform.*

**Solution:**

```{r}
load('dataex4.Rdata') # Load data
q4a_imps = mice(dataex4, m=50, seed=1, printFlag=FALSE)
q4a_ests = with(q4a_imps, lm(y ~ x1 + x2 + x1*x2))
summary(pool(q4a_ests), conf.int=TRUE)
```

## Question 4b
*Now, start by calculating the interaction variable in the incomplete data*
*and append it as a variable to your dataset. Then, use passive imputation to impute the*
*interaction variable. Provide the estimates o*f $\beta_1, \beta_2\; \text{and} \; \beta_3$ 
*along with $95\%$ confidence intervals. Comment.*
```{r}
# Create interaction variable (z=x1*x2)
dataex4$z = dataex4$x1 * dataex4$x2
# Initial MICE setup
imp_0_pass = mice(dataex4, maxit = 0, seed=1, m=50)
# Specify that z is derived from x1 and x3
meth_pass = imp_0_pass$method
meth_pass["z"] = "~I(x1*x2)"

pred_pass = imp_0_pass$predictorMatrix
pred_pass[c("x1", "x2"), "z"] = 0 # Don't use z to predict x1 and x2
pred_pass[, c("x1", "x2")] = 0 # Prevent multi-colinearity
# Restore imputations between x1 and x2
pred_pass["x1", "x2"] = 1 
pred_pass["x2", "x1"] = 1

# MI begins...
imp_pass = mice(dataex4, 
                method = meth_pass, 
                predictorMatrix = pred_pass, 
                m = 50, 
                seed = 1, 
                printFlag = FALSE)
imp_pass_ests = with(imp_pass, lm(y ~ x1 + x2 + z))
summary(pool(imp_pass_ests), conf.int=TRUE)
```

## Question 4c
*Now that you have already appended the interaction variable to the dataset,*
*impute it as it was just another variable (or like any other variable) in the dataset and*
*use this variable for the interaction term in step 2. Provide the estimates of* $\beta_1, \beta_2\; \text{and} \; \beta_3$  *along with 95% confidence intervals. Comment.*

```{r}
# Just another variable imputation
imps_jav = mice(dataex4, m=50, seed=1, printFlag=FALSE)
ests_jav = with(imps_jav, lm(y ~ x1 + x2 + z))
summary(pool(ests_jav), conf.int=TRUE)
```



## Question 4d
*What is the obvious conceptual drawback of the just another variable approach for* 
*imputing interactions?*

# Question 5
*The file* `NHANES2.Rdata` *contains a subset of data from the National Health*
*and Nutrition Examination Survey (NHANES), whose goal is to assess the health* 
*and nutritional status of adults and children in the United States.*

*The analysis of interest is the following:*

\begin{equation*}
  \text{wgt} = \beta_0 + \beta_1\text{gender} + \beta_3\text{hgt} + \beta_4\text{WC} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
\end{equation*}

*Using multiple imputation and conducting all necessary checks, report your findings.*


