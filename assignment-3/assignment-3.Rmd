---
title: |
  <center> Incomplete Data Analysis </center>
  <center> Assignment 2 </center>
author: "<center> Callum Abbott </center>"
output:
  pdf_document: default
  html_document: default
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE)
library(mice)
library(rjags)
library(JointAI)
```

All code used for this assignment can be found in the following repository [https://github.com/c-abbott/ida] under the folder `assignment-3`.

## Question 1
Consider the `nhanes` dataset in `mice`.

### Question 1a
*(a) What percentage of the cases are incomplete?*

**Solution:**
We observe the NHANES dataset has a total of $25$ cases, $12$ of which are
incomplete. This gives the incomplete case percentage as $48.0\%$.

```{r echo=TRUE}
# Load in dataset
data(nhanes)
# Calculate NA percentage
na_percentage = sum(!complete.cases(nhanes)) / nrow(nhanes) * 100
```

### Question 1b
*(b) Impute the data with `mice` using the defaults with `seed=1`, in step 2 predict*
*`bmi` from `age`, `hyp`, and `chl` by the normal linear regression model, and then pool the*
*results. What are the proportions of variance due to the missing data for each parameter?*
*Which parameters appear to be most affected by the non-response?*

**Solution:**

Let *the proportions of variance due to the missing data for each parameter* be denoted as $\lambda$. 

Mathematically, we specify $\lambda$ to be:
\begin{equation*}
  \lambda = \frac{B + \frac{B}{M}}{V^{\text{MI}}}
\end{equation*}

where $M$ denotes the number of imputed datasets by `mice` (default = 5), $B = \frac{1}{M-1}\sum_{m=1}^M\left(\hat{\theta}^{(m)}-\hat{\theta}^{MI}\right)^2$ denotes the *between-imputation* variance, and $V^{\text{MI}} = \bar{U} + (1 + 1/M)B$ is the *total variance*. Note that $\bar{U} = \frac{1}{M}\sum_{m=1}^M\hat{U}^{(m)}$ is known as the
*within-variance*.

Using the standard MICE procedure - `mice(), with(), pool()` - we yield the following
values for *the proportions of variance due to the missing data for each parameter* (3SF, seed=1):

* `age` = 0.686
* `hyp (positive)` = 0.350
* `chl` = 0.304

We observe that the variable `age` has the highest value of $\lambda$ indicating that
this variable is most affected by the non-response.

```{r}
imps = mice(nhanes, seed=1, printFlag=FALSE) # MI step
fits = with(imps, lm(bmi ~ age + hyp + chl)) # Fitting lm to predict BMI
bmi_ests = pool(fits) # Gather useful summary statistics
bmi_ests[,3][c(1, 3, 10)] # Extracting only the relevant statistics
```

### Question 1c 
*(c) Repeat the analysis for* `seed` $\in {2,3,4,5,6}$. *Do the conclusions remain the same?*

**Solution:**
Repeating an analogous analysis but now varying the seeds to the values given above we
yield different conclusions. For seeds 2, 3 and 6, the `age` variable had the largest
value of $\lambda$ with respective values of 0.403, 0.590 and 0.655.
Meanwhile the `chl` and `hyp` variables had the largest $\lambda$ values
for seeds 4 and 5 with values 0.331 and 0.594 respectively. Other numerical values have not been included for brevity purposes.

This behaviour can be explained by the fact that $48\%$ of cases our are missing in the
NHANES dataset. Therefore, as we adjust the random seed, the proportion of variance due to the missing data can change significantly since it occupies such a large proportion of our
total data and hence leads us to conflicting conclusions.

```{r}
bmi2 = pool(with(mice(nhanes, seed=2, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi3 = pool(with(mice(nhanes, seed=3, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi4 = pool(with(mice(nhanes, seed=4, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi5 = pool(with(mice(nhanes, seed=5, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
bmi6 = pool(with(mice(nhanes, seed=6, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
```

### Question 1d
*(d) Repeat the analysis with M = 100 with the same seeds. Would you prefer*
*these analyses over those with M = 5? Explain why.*

**Solution:**
Given that the conclusions derived in 1(c) varied with the random seed, this indicates 
we must increase the value of $M$ (the number of imputed datasets) in order to reduce
the probability of our conclusions being governed by random chance.

We set $M=100$ and repeat the analysis outlined in 1(b) and 1(c). We now find that `age`
is the parameter most affected by the non-response (largest $\lambda$) in 5 of the 6 seeds
with `chl` taking the same stance for seed 3.

Overall, I would favour the conclusions derived from the $M=100$ results since
if we analyze the expression of the *total variance* of the imputed parameter estimates
(in this case `bmi`),

\begin{equation*}
  V^{\text{MI}} = \bar{U} + B + \frac{B}{M}
\end{equation*}

where $\bar{U}$ is the *within-imputation variance* and $B$ is again the 
*between-imputation variance*, we see that increasing $M$ has the effect of reducing
this quantity thus increasing the reliability of estimations. It should be noted that both
$B$ and $\bar{U}$ are also  $\propto \frac{1}{M}$ which further reinforces this notion. 

On the contrary, a larger $M$ results in a lower level of statistical efficiency i.e.
one should be able to arrive at the same statistically significant conclusions
using a lesser amount of computational resources and time. In this context, $M=100$ is
easily handled by most modern machines given the NHANES dataset contains only 25 cases - a low
burden compared to standards of modern datasets.

```{r}
#mbmi1 = pool(with(mice(nhanes, seed=1, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi2 = pool(with(mice(nhanes, seed=2, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi3 = pool(with(mice(nhanes, seed=3, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi4 = pool(with(mice(nhanes, seed=4, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi5 = pool(with(mice(nhanes, seed=5, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
#mbmi6 = pool(with(mice(nhanes, seed=6, m=100, printFlag=FALSE), lm(bmi ~ age + hyp + chl)))
```

# Question 2
*Each of the 100 datasets contained in `dataex2.Rdata` was generated in the* 
*following way*
\begin{equation*}
 y_i | x_i \stackrel{\text{ind.}}{\sim} \mathcal{N}(\beta_0+\beta_1x_i,1), \quad x_i \stackrel{\text{iid}}{\sim} \text{Unif}(-1, 1), \quad \beta_0 = 1, \quad \beta_1 = 3
\end{equation*}
*for* $i = 1,...,100$. *Additionally, some of the responses were set to be missing using a*
*MAR mechanism.* 

*The goal of this exercise is to study the effect that acknowledging/not*
*acknowledging parameter uncertainty when performing step 1 of multiple imputation might*
*have on the coverage of the corresponding confidence intervals. Further suppose that the*
*analysis of interest in step 2 is to fit the regression model that was used to generate the data,*
*i.e., a normal linear regression model where the response is* $y$ *and the covariate is* $x$.

*With the*
*aid of the mice package, calculate the empirical coverage probability of the* $95\%$ *confidence*
*intervals for* $\beta_1$ *under the following two approaches: stochastic regression imputation and*
*the corresponding bootstrap based version. Comment. For both approaches, please consider*
$M = 20$ *and* $\text{seed}=1$

**Solution:**
The R code below compares two methods used to impute missing values in incomplete datasets
through comparing empirical confidence intervals for the $\beta_1$ parameter -
these methods are stochastic regression imputation (SRI) and bootstrap sampling.

We remind the reader how confidence intervals (CIs) of an unknown parameter are interpreted 
in the frequentist paradigm of statistics. Upon collecting a sample of data, 
a $\text{p}\%$ CI can be constructed from this data. If such intervals were constructed
repeatedly from collecting more samples of the data, the $\text{p}\%$ CI will contain
the unknown parameter $\text{p}\%$ of the time.

The R code below hence performs a bootstrap and SRI imputation
method for each of the 100 datasets contained withing `dataex2.Rdata`. Once the
missing data has been imputed, we derive the $95\%$ CIs for each method and
check whether the true value of $\beta_1 = 3$ is contained within
the bootstrap, or SRI, $95\%$ CI. Given the CI interpretation outlined above, one
would expect the true value of $\beta_1$ to be contained within CIs $95\%$ of
the time for both imputation methods.

In reality, we observe this is true for the bootstrap method (`count_boot = 95`)
but not for the SRI method whose CIs only contain the true value of $\beta_1$ 
$88\%$ of the time (`count_sri = 88`). This is because, unlike the bootstrap imputation
method, SRI does not take into consideration the uncertainty surrounding the 
imputed values. More simply put, the SRI method treats the imputed values as if
they were truly observed rather than imputations from missing values.
This treatment is unjustified and hence results in the construction of more narrow, 
overly-confident CIs surrounding $\beta_1$ which are less likely to contain
the true value of $\beta_1$.

```{r}
load('dataex2.Rdata') # Loading in datasets

# Counters to track empirical coverage probability
count_sri = 0
count_boot = 0

# Imputations begin...
for (dataset in 1:2){
  # SRI & Bootstrap MICE methods initialized
  imps_sri = mice(dataex2[,,dataset], m=20, seed=1, printFlag=FALSE, method="norm.nob")
  imps_boot = mice(dataex2[,,dataset], m=20, seed=1, printFlag=FALSE, method="norm.boot")
  # Grab 95% CIs
  stats_sri = summary(pool(with(imps_sri, lm(Y ~ X))), conf.int=TRUE)
  stats_boot = summary(pool(with(imps_boot, lm(Y ~ X))), conf.int=TRUE)
  # Checking whether true value (3) of beta1 is in the CIs
  if (stats_sri$`2.5 %`[2] <= 3 & stats_sri$`97.5 %`[2] >= 3){
    count_sri = count_sri + 1
  }
  if (stats_boot$`2.5 %`[2] <= 3 & stats_boot$`97.5 %`[2] >= 3){
    count_boot = count_boot + 1
  } 
}
```


# Question 3
*Show that for a linear (in the coefficients) regression model, the following two*
*strategies coincide:*

* Method 1: *Computing the predicted values (point estimates) from each fitted model in step 2 and*
   *then pooling them according to Rubin’s rule for point estimates (i.e., averaging the*
    *predicted values across the imputed datasets).*
      
* Method 2: *Pooling the regression coefficients from each fitted model in step 2 using Rubin’s rule*
   *for point estimates and then computing the predicted values afterwards.*
       
**Solution:**

Let us begin by first defining a general linear (in the coefficients) regression
model with a total of $N$ covariates to act as our substantive model in the multiple
imputation process ($\epsilon \sim \mathcal{N}(0, \sigma^2)$).

\begin{align*}
  y &= \sum_{i=0}^N \beta_ix_i + \epsilon \\
  &= \beta_0 + \sum_{i=1}^N \beta_ix_i + \epsilon
\end{align*}

Let us assume that step 1 of the multiple imputation process has been performed
and we are in possession of a total of $M$ imputed datasets. We fit the previously
defined regression model to each of these $M$ datasets yielding a set of predictions, $\mathbf{\hat{y}} = (\hat{y}^{(1)},\hat{y}^{(2)},...,\hat{y}^{(M)})$. Method 1 instructs us to pool these predictions according to Rubin's rule for point estimates yielding,

\begin{align*}
  \tilde{y} &= \frac{1}{M}\sum_{m=1}^M\color{orange}{\hat{y}^{(m)}}\\
  &= \frac{1}{M}\sum_{m=1}^M\color{orange}{\sum_{i=0}^N \beta_i^{(m)}x_i}\\
  &= \color{red}{\sum_{i=0}^Nx_i}\color{blue}{\frac{1}{M}\sum_{m=1}^M\beta_i^{(m)}}\\
  &= \color{red}{\sum_{i=0}^N}\color{blue}{\tilde{\beta}_i}\color{red}{x_i}
\end{align*}

where $\tilde{\beta}_i=\frac{1}{M}\sum_{m=1}^M\beta_i^{(m)}$ - **the pooled regression
coefficients according to Rubin's rules (Method 2)**.

The equations derived above hence indicate that Method 1 and Method 2 are mathematically
equivalent and will lead to the same results and conclusions.

# Question 4 
*The goal of this exercise is to study different ways of using* `mice` *when the analysis model*
*of interest/substantive model involves an interaction term between incomplete variables. The*
*model used to generate the data (available in* `dataex4.Rdata`), *which corresponds to our*
*model of interest in step 2, was the following one:*

\begin{align*}
  y_i &= \beta_0 + \beta_1x_{1i} + \beta_2x_{2i} + \beta_3x_{1i}x_{2i} + \epsilon_i,\\
  x_{1i} &\stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1), \quad x_{2i} \stackrel{\text{iid}}{\sim} \mathcal{N}(1.5, 1), \quad \epsilon_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)
\end{align*}
*for* $i=1,...,1000, \beta_0=1.5, \beta_1=1, \beta_2=2, \beta_3=1$.

*Additionally, missingness was*
*imposed on* $y$ *and* $x_1$ *and so the interaction variable* $x_1x_2$ *also has missing values, although*
*the missingness in this interaction variable is induced by the missing in the covariate* $x_1$.

## Question 4a
*By only imputing the* $y$ *and* $x_1$ *variables in step 1, provide the estimates of*
$\beta_1, \beta_2\; \text{and} \; \beta_3$ *along with 95% confidence intervals. Comment. Note that this approach*
*where the interaction variable is left outside the imputation process and calculated afterwards in the analysis* *model, is known as Impute, then transform.*

**Solution:**

Solution 4a sees us perform the *impute, then transform* method of imputation
where we choose to explicitly impute $x_1$ and $y$ using MICE, and then use the 
imputed $x_1$ values to passively impute the interaction term, $x_1x_2$, in our 
substantive model.

Using a value of $M=50$, `seed=1`  and `maxit=5` we obtain the following estimates and $95\%$
CIs for $\beta_1$, $\beta_2$, and $\beta_3$ (3SF):

* $\beta_1 = 1.41;\quad$        $95\%\text{CI}=[1.22, 1.60]$
* $\beta_2 = 1.97;\quad$        $95\%\text{CI}=[1.86, 2.07]$
* $\beta_3 = 0.75;\quad$       $95\%\text{CI}=[0.642, 0.868]$

Analyzing these results we observe that only the 95% CI associated with $\beta_2$
contains the true value of the parameter ($\beta_2=2$) and a reasonable estimate. 
This is however rather meaningless since the $x_2$ variable was already fully observed and never
imputed.


Meanwhile, we observe that the 95% CIs associated with $\beta_1$ and $\beta_3$ fail
to include the true values of these parameters ($\beta_1=1$, $\beta_3 =1$) and provide
relatively inaccurate estimates. This is unsurprising behaviour since the impute then 
transform method (in general) is known to lead biased estimates (*Hippel, 2009*).


```{r}
load('dataex4.Rdata') # Load data
q4a_imps0 = mice(dataex4, m=50, seed=1, printFlag=FALSE, maxit=0) # Initial MICE
q4a_imps0$predictorMatrix["x2",] = 0 # Explicitly preventing x2 from being imputed

# Running MICE with new predictorMatrix
q4a_imps = mice(dataex4, m=50, seed=1, printFlag=FALSE,
                predictorMatrix=q4a_imps0$predictorMatrix) 
q4a_ests = with(q4a_imps, lm(y ~ x1 + x2 + x1*x2)) # Getting estimates
summary(pool(q4a_ests), conf.int=TRUE)[-1,c(1,2,7,8)] # Extracting relevant statistics
```


## Question 4b
*Now, start by calculating the interaction variable in the incomplete data*
*and append it as a variable to your dataset. Then, use passive imputation to impute the*
*interaction variable. Provide the estimates of* $\beta_1, \beta_2\; \text{and} \; \beta_3$ 
*along with $95\%$ confidence intervals. Comment.*


**Solution:**

Solution 4b sees us add the interaction term to our dataset, $z = x_1x_2$,
and use passive imputation to impute the missing values (maintaining the
deterministic relationship).

Using a value of $M=50$, `seed=1` and `maxit=5` we obtain the following estimates and $95\%$
CIs for $\beta_1$, $\beta_2$, and $\beta_3$ (3SF):

* $\beta_1 = 1.19;\quad$        $95\%\text{CI}=[1.0035, 1.38]$
* $\beta_2 = 2.00;\quad$        $95\%\text{CI}=[1.90, 2.09]$
* $\beta_3 = 0.874;\quad$       $95\%\text{CI}=[0.762, 0.987]$

Using this imputation method we now observe an increase in the accuracy of the estimates
made for all three parameters. In addition, although the 95% CIs for $\beta_1$ and
$\beta_3$ still do not include the true value of these parameter, they
are much closer to doing so than the method implemented in Q4a. Passive imputation,
in this context, has hence resulted in a reduction in the bias of our estimates made by 
our substantive model but not eradicated the problem.

```{r}
# Create interaction variable (z=x1*x2)
dataex4$z = dataex4$x1 * dataex4$x2
# Initial MICE setup
imp_0_pass = mice(dataex4, maxit = 0, seed=1, m=50)
# Specify that z is derived from x1 and x3
meth_pass = imp_0_pass$method
meth_pass["z"] = "~I(x1*x2)"

pred_pass = imp_0_pass$predictorMatrix
pred_pass[c("x1", "x2"), "z"] = 0 # Don't use z to impute x1 and x2
pred_pass["z", "y"] = 0 # Don't use y to impute z

# MI begins...
imp_pass = mice(dataex4, 
                method = meth_pass, 
                predictorMatrix = pred_pass, 
                m = 50, 
                seed = 1, 
                printFlag = FALSE)
imp_pass_ests = with(imp_pass, lm(y ~ x1 + x2 + z))
summary(pool(imp_pass_ests), conf.int=TRUE)[-1,c(1,2,7,8)] 
```

## Question 4c
*Now that you have already appended the interaction variable to the dataset,*
*impute it as it was just another variable (or like any other variable) in the dataset and*
*use this variable for the interaction term in step 2. Provide the estimates of* $\beta_1, \beta_2\; \text{and} \; \beta_3$  *along with 95% confidence intervals. Comment.*

**Solution:**

We now treat the interaction term, $z = x_1x_2$, as if it were *just another variable*
(JAV) meaning that missing values of $z$ are imputed independently of missing
$x_1$ values.

Repeating the procedure performed in 4a and 4b with $M=50$, `seed=1` and `maxit=5`,
we obtain the following estimates and $95\%$ CIs for $\beta_1$, $\beta_2$, and $\beta_3$ (3SF):

* $\beta_1 = 1.0039;\quad$        $95\%\text{CI}=[0.841, 1.17]$
* $\beta_2 = 2.03;\quad$        $95\%\text{CI}=[1.94, 2.11]$
* $\beta_3 = 1.02;\quad$       $95\%\text{CI}=[0.930, 1.105]$

We now observe in the regime of treating the interaction term as just another
variable yields estimates which are accurately to at least 1 decimal place
and all three 95% CIs contain the true parameter values. This is a significant
improvement to what was observed in 4a and 4b.

We can explain this result by the fact that although the substantive model is 
incorrectly specified (regarding the inconsistency in imputed vales) the imputations 
created for all variables do in fact have the correct means and covariances. We
recall that the parameters of our substantive model (the coefficients) depend
only on these imputed values, and hence result in a set of unbiased estimates for the missing
data values.

```{r}
# Just another variable imputation
imps_jav = mice(dataex4, m=50, seed=1, printFlag=FALSE)
ests_jav = with(imps_jav, lm(y ~ x1 + x2 + z))
summary(pool(ests_jav), conf.int=TRUE)[-1,c(1,2,7,8)]
```



## Question 4d
*What is the obvious conceptual drawback of the just another variable approach for* 
*imputing interactions?*

The cost of obtaining unbiased estimates for the parameters of our linear regression
model is that the deterministic relationship for the interaction term, $z=x_1x_2$,
no longer holds.

Re-iterating this point more explicitly, when we impute missing values of $z$ independently
of $x_1$, we remove the deterministic dependence between these two variables and hence
$z \neq x_1x_2$ for the imputed $z$ values.

# Question 5
*The file* `NHANES2.Rdata` *contains a subset of data from the National Health*
*and Nutrition Examination Survey (NHANES), whose goal is to assess the health* 
*and nutritional status of adults and children in the United States.*

*The analysis of interest is the following:*

\begin{equation*}
  \text{wgt} = \beta_0 + \beta_1\text{gender} + \beta_3\text{hgt} + \beta_4\text{WC} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
\end{equation*}

*Using multiple imputation and conducting all necessary checks, report your findings.*

```{r}
load('NHANES2.Rdata')
dim(NHANES2)
md_pattern(NHANES2, pattern = FALSE, color = c('#34111b', '#e30f41'))
```

```{r}
# Does MICE handles NaNs
# Passive imputation between SBP and Hypternsive, maybe?
# Correlation
par(mar = c(3, 3, 2, 1), mgp = c(2, 0.6, 0))
plot_all(NHANES2, breaks = 30, ncol = 4)
```

```{r}
#q5_imps = mice(NHANES2, m=5, seed=1, printFlag=FALSE)
#with(q5_imps, lm(y ~ x1 + x2 + z))
#summary(pool(ests_jav), conf.int=TRUE)
```


## References
Hippel, P.T., 2009. How to Impute Interactions, Squares, and Other Transformed Variables. Sociological methodology, 39(1), pp.265–291.

